{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Tables for Statistical Analysis with R\n",
    "\n",
    "* For statistical modeling with a train and a test dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated 12 December 2025\n",
      "Author: Sylvain Haupert\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Updated 12 December 2025\n",
    "Author: Sylvain Haupert\n",
    "\"\"\"\n",
    "\n",
    "from IPython import get_ipython\n",
    "print(__doc__)\n",
    "\n",
    "# Clear all the variables\n",
    "get_ipython().run_line_magic('reset', '-sf')\n",
    "\n",
    "# suppress all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pathlib import Path \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "pd.options.display.float_format = \"{:,.2f}\".format # display numbers with 2 decimals\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from maad import util\n",
    "\n",
    "sys.path.append(str(Path('../src')))\n",
    "from stat_func import bootstrap_corr, permutation_corr\n",
    "\n",
    "# Close all the figures (like in Matlab)\n",
    "plt.close(\"all\")\n",
    "\n",
    "# Import configuration file\n",
    "import config as cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Notebook init and options setting\n",
    "\n",
    "* Select the annotation file\n",
    "* Select the dataset to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sapsucker_woods is being preparing...\n",
      "The dataset sapsucker_woods contains 1 sites \n",
      "\n",
      "Dataset bialowieza is being preparing...\n",
      "The dataset bialowieza contains 15 sites \n",
      "\n",
      "Dataset hawai is being preparing...\n",
      "The dataset hawai contains 2 sites \n",
      "\n",
      "Dataset coffee_farms is being preparing...\n",
      "The dataset coffee_farms contains 2 sites \n",
      "\n",
      "Dataset usa_sierra_nevada_forest is being preparing...\n",
      "The dataset usa_sierra_nevada_forest contains 33 sites \n",
      "\n",
      "Dataset uk_sussex_countryside is being preparing...\n",
      "The dataset uk_sussex_countryside contains 45 sites \n",
      "\n",
      "Dataset ecuador is being preparing...\n",
      "The dataset ecuador contains 45 sites \n",
      "\n",
      "Dataset risoux is being preparing...\n",
      "The dataset risoux contains 1 sites \n",
      "\n",
      "Dataset peru is being preparing...\n",
      "The dataset peru contains 7 sites \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"****************************************************************************\n",
    "                                   options          \n",
    "****************************************************************************\"\"\"\n",
    "\n",
    "# Set the options\n",
    "SAVE = True\n",
    "\n",
    "# Load the configuration file\n",
    "# CONFIG = cfg.load_config('config_publication.yaml')\n",
    "CONFIG = cfg.load_config('config_publication_local.yaml')\n",
    "\n",
    "# initialize the list of indices, annotations and sites\n",
    "INDICES_ALL = []\n",
    "ANNOTATIONS_ALL = []\n",
    "SITES_ALL = []\n",
    "\n",
    "# loop over the datasets to get the filenames of the indices and annotations and the selected sites corresponding to each dataset\n",
    "for DATASET in CONFIG['datasets']:\n",
    "\n",
    "       \"\"\"***************************************************************************\n",
    "                     Prepare the configuration for the dataset\n",
    "       ***************************************************************************\"\"\"\n",
    "\n",
    "       filename_indices = 'indices_'+DATASET['name']+'_BW'+str(DATASET['flim_min'])+'Hz_'+str(DATASET['flim_max'])+'Hz_'+str(CONFIG['seed_level'])+'db'+'.csv'\n",
    "\n",
    "       INDICES_ALL     += [(filename_indices, CONFIG['save_dir'])]\n",
    "       ANNOTATIONS_ALL += [(CONFIG['annotations_filename'], os.path.dirname(DATASET['path']))] \n",
    "       SITES_ALL       += DATASET['sites']\n",
    "\n",
    "       # display the dataset name\n",
    "       print(f'Dataset {DATASET[\"name\"]} is being preparing...')       \n",
    "       # display the number of sites\n",
    "       print(f'The dataset {DATASET[\"name\"]} contains {len(DATASET[\"sites\"])} sites \\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparation of the dataframe df\n",
    "\n",
    "* Open and read dataframes with indices (variables) and with labels (ground truth)\n",
    "* Clean and merge both dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Clean, merge and save the dataframe with all the data\n",
    "\n",
    "* Indices where previously calculated with scikit-maad package (V1.5.1) on Python.\n",
    "* Manual annotations are coming from different datasets that belong to very different projects. They were formated to be readable in the same way.\n",
    "\n",
    "> Indices and manual annotations are merged into a single dataframe. Some variables are renamed while others are dropped because we do not want to analyse them. The final dataframe is saved for further analyses, for instance in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of files after removing clipped audio\n",
      "The total number of files 4484\n",
      "The number of files for the dataset sapsucker_woods is 544\n",
      "The number of files for the dataset bialowieza is 540\n",
      "The number of files for the dataset hawai is 357\n",
      "The number of files for the dataset coffee_farms is 112\n",
      "The number of files for the dataset usa_sierra_nevada_forest is 132\n",
      "The number of files for the dataset uk_sussex_countryside is 1982\n",
      "The number of files for the dataset ecuador is 675\n",
      "The number of files for the dataset risoux is 58\n",
      "The number of files for the dataset peru is 84\n",
      "\n",
      "\n",
      "The number of files for the habitat Forest - Temperate is 1949\n",
      "The number of sites for the habitat Forest - Temperate is 65 \n",
      "\n",
      "The number of files for the habitat Shrubland - Subtropical-tropical high altitude is 227\n",
      "The number of sites for the habitat Shrubland - Subtropical-tropical high altitude is 1 \n",
      "\n",
      "The number of files for the habitat Forest - Subtropical-tropical moist montane is 355\n",
      "The number of sites for the habitat Forest - Subtropical-tropical moist montane is 16 \n",
      "\n",
      "The number of files for the habitat Plantations is 337\n",
      "The number of sites for the habitat Plantations is 17 \n",
      "\n",
      "The number of files for the habitat Shrubland - Temperate is 675\n",
      "The number of sites for the habitat Shrubland - Temperate is 15 \n",
      "\n",
      "The number of files for the habitat Arable land is 632\n",
      "The number of sites for the habitat Arable land is 15 \n",
      "\n",
      "The number of files for the habitat Forest - Subtropical-tropical moist lowland is 309\n",
      "The number of sites for the habitat Forest - Subtropical-tropical moist lowland is 22 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"****************************************************************************\n",
    "                                Dataframe creation  \n",
    "****************************************************************************\"\"\"\n",
    "# LOAD CSV\n",
    "df_indices = pd.DataFrame()\n",
    "for indices_csv, indices_dir in INDICES_ALL :\n",
    "    df_indices = pd.concat([df_indices, pd.read_csv(os.path.join(indices_dir,indices_csv), sep=',')], axis=0)\n",
    "\n",
    "df_label = pd.DataFrame()\n",
    "for annotations_csv, annotations_dir in ANNOTATIONS_ALL :\n",
    "    # add a column dataset containing the dataset name (last  part of the path in annotations_dir)\n",
    "    df_temp = pd.read_csv(os.path.join(annotations_dir,annotations_csv), sep=',')\n",
    "    df_temp['dataset'] = annotations_dir.split('/')[-1]\n",
    "    df_label = pd.concat([df_label, df_temp], axis=0)\n",
    "    \n",
    "# select a site\n",
    "df_label = df_label[df_label['site'].isin(SITES_ALL)]\n",
    "\n",
    "# add a column Filename such as in df_label\n",
    "df_indices['filename'] = df_indices['file'].apply(lambda x: os.path.basename(x).split('.')[0])\n",
    "df_indices.drop(['Date','file','clipping'], axis='columns', inplace=True)\n",
    "\n",
    "# set index\n",
    "df_indices.set_index('filename', inplace = True)\n",
    "df_label.set_index('filename', inplace = True)\n",
    "\n",
    "# keep several columns\n",
    "df_label = df_label[[ 'date', 'site', 'LAT', 'LON', 'device_id', 'biome', 'dataset', 'species_richness']]\n",
    "\n",
    "# Rename the column biome into habitat\n",
    "df_label = df_label.rename(columns={'biome':'habitat'})\n",
    "\n",
    "# HACK\n",
    "df_label['species_richness'] = df_label['species_richness'].astype('float')\n",
    "\n",
    "# Remove  ENRf which is the same as LEQf\n",
    "try :\n",
    "    df_indices.drop(['ENRf', 'audio_duration'], axis='columns', inplace=True)\n",
    "except :\n",
    "    pass\n",
    "\n",
    "# Create H indice, a composite of Ht and Hf\n",
    "df_indices['H'] = df_indices['Ht'] * df_indices['Hf']\n",
    "# Rename some indices\n",
    "df_indices = df_indices.rename(columns={'NBPEAKS':'NP'})\n",
    "df_indices = df_indices.rename(columns={'BI':'BIO'})\n",
    "df_indices = df_indices.rename(columns={'MED':'M'})\n",
    "\n",
    "# transform the values into float\n",
    "df_indices = df_indices.select_dtypes(include=['number']).astype('float')\n",
    "\n",
    "# Create a new df by merging df_indices and df_label\n",
    "df_train_dataset_for_stat_in_R = pd.merge(df_indices, df_label, on='filename', how='inner')\n",
    "\n",
    "if CONFIG['remove_clipping_audio'] :\n",
    "    print('The number of files after removing clipped audio')\n",
    "\n",
    "print('The total number of files {}'.format(len(df_train_dataset_for_stat_in_R)))\n",
    "\n",
    "# display the number of files per dataset\n",
    "for dataset in CONFIG['datasets'] :\n",
    "    print(f'The number of files for the dataset {dataset[\"name\"]} is {len(df_train_dataset_for_stat_in_R[df_train_dataset_for_stat_in_R[\"site\"].isin(dataset[\"sites\"])])}')\n",
    "\n",
    "# display the number of files per habitat and the number of sites per habitat\n",
    "print('\\n')\n",
    "for habitat in df_train_dataset_for_stat_in_R['habitat'].unique() :\n",
    "    print(f'The number of files for the habitat {habitat} is {len(df_train_dataset_for_stat_in_R[df_train_dataset_for_stat_in_R[\"habitat\"] == habitat])}')\n",
    "    print(f'The number of sites for the habitat {habitat} is {len(df_train_dataset_for_stat_in_R[df_train_dataset_for_stat_in_R[\"habitat\"] == habitat][\"site\"].unique())} \\n')\n",
    "\n",
    "# save the dataframe\n",
    "if SAVE :\n",
    "    df_train_dataset_for_stat_in_R.to_csv(path_or_buf=os.path.join(CONFIG['save_dir'], 'train_dataset_for_statistical_modeling_in_R.csv'), sep=',', mode='w', header=True, index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wabad Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"****************************************************************************\n",
    "# -------------------          options              ---------------------------\n",
    "****************************************************************************\"\"\"\n",
    "SAVE = True\n",
    "\n",
    "# Load the configuration file\n",
    "# CONFIG = cfg.load_config('config_publication_wabad.yaml')\n",
    "CONFIG = cfg.load_config('config_publication_local_wabad.yaml')\n",
    "\n",
    "# initialize the list of indices, annotations and sites\n",
    "INDICES_ALL = []\n",
    "ANNOTATIONS_ALL = []\n",
    "SITES_ALL = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset wabad is being preparing...\n",
      "The dataset wabad contains 70 sites \n",
      "\n",
      "The total number of files 1552\n",
      "The number of files for the dataset wabad is 1552\n",
      "\n",
      "\n",
      "The number of files for the habitat Forest - Subtropical-tropical moist montane is 208\n",
      "The number of sites for the habitat Forest - Subtropical-tropical moist montane is 8 \n",
      "\n",
      "The number of files for the habitat Shrubland - Subtropical-tropical high altitude is 34\n",
      "The number of sites for the habitat Shrubland - Subtropical-tropical high altitude is 5 \n",
      "\n",
      "The number of files for the habitat Forest - Subtropical-tropical dry is 178\n",
      "The number of sites for the habitat Forest - Subtropical-tropical dry is 11 \n",
      "\n",
      "The number of files for the habitat Forest - Subtropical-tropical moist lowland is 175\n",
      "The number of sites for the habitat Forest - Subtropical-tropical moist lowland is 9 \n",
      "\n",
      "The number of files for the habitat Shrubland - Temperate is 71\n",
      "The number of sites for the habitat Shrubland - Temperate is 3 \n",
      "\n",
      "The number of files for the habitat Forest - Temperate is 84\n",
      "The number of sites for the habitat Forest - Temperate is 4 \n",
      "\n",
      "The number of files for the habitat Grassland - Temperate is 39\n",
      "The number of sites for the habitat Grassland - Temperate is 3 \n",
      "\n",
      "The number of files for the habitat Forest - Boreal is 52\n",
      "The number of sites for the habitat Forest - Boreal is 5 \n",
      "\n",
      "The number of files for the habitat Shrubland - Subtropical-tropical dry is 31\n",
      "The number of sites for the habitat Shrubland - Subtropical-tropical dry is 1 \n",
      "\n",
      "The number of files for the habitat Pastureland is 80\n",
      "The number of sites for the habitat Pastureland is 3 \n",
      "\n",
      "The number of files for the habitat Arable land is 64\n",
      "The number of sites for the habitat Arable land is 3 \n",
      "\n",
      "The number of files for the habitat Wetland (inland) is 144\n",
      "The number of sites for the habitat Wetland (inland) is 6 \n",
      "\n",
      "The number of files for the habitat Savanna - Dry is 39\n",
      "The number of sites for the habitat Savanna - Dry is 2 \n",
      "\n",
      "The number of files for the habitat Plantations is 218\n",
      "The number of sites for the habitat Plantations is 3 \n",
      "\n",
      "The number of files for the habitat Shrubland - Mediterranean-type is 115\n",
      "The number of sites for the habitat Shrubland - Mediterranean-type is 3 \n",
      "\n",
      "The number of files for the habitat Desert - Hot is 20\n",
      "The number of sites for the habitat Desert - Hot is 1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# loop over the datasets to get the filenames of the indices and annotations and the selected sites corresponding to each dataset\n",
    "for DATASET in CONFIG['datasets']:\n",
    "\n",
    "    \"\"\"***************************************************************************\n",
    "                    Prepare the configuration for the dataset\n",
    "    ***************************************************************************\"\"\"\n",
    "\n",
    "    filename_indices = 'indices_'+DATASET['name']+'_BW'+str(DATASET['flim_min'])+'Hz_'+str(DATASET['flim_max'])+'Hz_'+str(CONFIG['seed_level'])+'db'+'.csv'\n",
    "\n",
    "    INDICES_ALL     += [(filename_indices, CONFIG['save_dir'])]\n",
    "    ANNOTATIONS_ALL += [(CONFIG['annotations_filename'], os.path.dirname(DATASET['path']))] \n",
    "    SITES_ALL       += DATASET['sites']\n",
    "\n",
    "    # display the dataset name\n",
    "    print(f'Dataset {DATASET[\"name\"]} is being preparing...')       \n",
    "    # display the number of sites\n",
    "    print(f'The dataset {DATASET[\"name\"]} contains {len(DATASET[\"sites\"])} sites \\n')\n",
    "\n",
    "\n",
    "\"\"\"****************************************************************************\n",
    "                                Dataframe creation  \n",
    "****************************************************************************\"\"\"\n",
    "# LOAD CSV\n",
    "df_indices = pd.DataFrame()\n",
    "for indices_csv, indices_dir in INDICES_ALL :\n",
    "    df_indices = pd.concat([df_indices, pd.read_csv(os.path.join(indices_dir,indices_csv), sep=',')], axis=0)\n",
    "\n",
    "df_label = pd.DataFrame()\n",
    "for annotations_csv, annotations_dir in ANNOTATIONS_ALL :\n",
    "    df_label   = pd.concat([df_label, pd.read_csv(os.path.join(annotations_dir,annotations_csv), sep=',')], axis=0)\n",
    "\n",
    "# select a site\n",
    "df_label = df_label[df_label['site'].isin(SITES_ALL)]\n",
    "\n",
    "# drop clipping rows\n",
    "if CONFIG['remove_clipping_audio'] :\n",
    "    df_indices = df_indices[df_indices['clipping'] == 0]\n",
    "\n",
    "# add a column Filename such as in df_label\n",
    "df_indices['filename'] = df_indices['file'].apply(lambda x: os.path.basename(x).split('.')[0])\n",
    "df_indices.drop(['Date','file','clipping'], axis='columns', inplace=True)\n",
    "\n",
    "# set index\n",
    "df_indices.set_index('filename', inplace = True)\n",
    "df_label.set_index('filename', inplace = True)\n",
    "\n",
    "# keep only richness ('species_richness')\n",
    "df_label = df_label[[ 'date', 'site', 'device_id', 'biome','species_richness']]\n",
    "\n",
    "# Rename the column biome into habitat\n",
    "df_label = df_label.rename(columns={'biome':'habitat'})\n",
    "\n",
    "# HACK\n",
    "df_label['species_richness'] = df_label['species_richness'].astype('float')\n",
    "\n",
    "# Remove  ENRf which is the same as LEQf\n",
    "try :\n",
    "    df_indices.drop(['ENRf', 'audio_duration'], axis='columns', inplace=True)\n",
    "except :\n",
    "    pass\n",
    "\n",
    "# Create H indice, a composite of Ht and Hf\n",
    "df_indices['H'] = df_indices['Ht'] * df_indices['Hf']\n",
    "# Rename isome indices\n",
    "df_indices = df_indices.rename(columns={'NBPEAKS':'NP'})\n",
    "df_indices = df_indices.rename(columns={'BI':'BIO'})\n",
    "df_indices = df_indices.rename(columns={'MED':'M'})\n",
    "\n",
    "# transform the values into float\n",
    "df_indices = df_indices.select_dtypes(include=['number']).astype('float')\n",
    "\n",
    "# Create a new df by merging df_indices and df_label\n",
    "df = pd.merge(df_indices, df_label, on='filename', how='inner')\n",
    "\n",
    "print('The total number of files {}'.format(len(df)))\n",
    "\n",
    "# display the number of files per dataset\n",
    "for dataset in CONFIG['datasets'] :\n",
    "    print(f'The number of files for the dataset {dataset[\"name\"]} is {len(df[df[\"site\"].isin(dataset[\"sites\"])])}')\n",
    "\n",
    "# display the number of files per habitat and the number of sites per habitat\n",
    "print('\\n')\n",
    "for habitat in df['habitat'].unique() :\n",
    "    print(f'The number of files for the habitat {habitat} is {len(df[df[\"habitat\"] == habitat])}')\n",
    "    print(f'The number of sites for the habitat {habitat} is {len(df[df[\"habitat\"] == habitat][\"site\"].unique())} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataframe for GLMM model has been saved to ../results/test_dataset_for_statistical_modeling_in_R.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"------------------------------------------------------------------------\n",
    "Group data and compute the correlation the indice and the ground truth. \n",
    "-------------------------------------------------------------------------\"\"\"\n",
    "\n",
    "df_save_for_stat_in_R = df.copy()\n",
    "\n",
    "# Use the groupby method to group by the 'site' column\n",
    "grouped = df_save_for_stat_in_R.groupby('site')\n",
    "\n",
    "# Filter numeric columns\n",
    "numeric_columns = df.select_dtypes(include=[int, float])\n",
    "\n",
    "# Group by 'site' and calculate the mean for each numeric column\n",
    "df_test_dataset_for_stat_in_R_count= grouped[numeric_columns.columns].count()\n",
    "df_test_dataset_for_stat_in_R_mean = grouped[numeric_columns.columns].mean()\n",
    "\n",
    "# add a count column to keep the nummber of 1min audio files by site\n",
    "df_test_dataset_for_stat_in_R_mean['count'] = df_test_dataset_for_stat_in_R_count.iloc[:,0]\n",
    "\n",
    "# save the dataframe for stat in R\n",
    "if SAVE:\n",
    "    new_data_path = os.path.join(CONFIG['save_dir'], 'test_dataset_for_statistical_modeling_in_R.csv')\n",
    "    df_test_dataset_for_stat_in_R_mean.to_csv(new_data_path, index=True)\n",
    "    print(f'The dataframe for GLMM model has been saved to {new_data_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ear-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
